{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinky and the Brain: Transforming and Translating Neural Activity with Neural Networks\n",
    "\n",
    "Predicting mice behavior by translating neural data encoding to behavioral decodings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*note: the following are packages used in this project.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from Transformer import Transformer\n",
    "from positional_encoding import PositionalEncoding\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from roc_auc import cal_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Plan: \n",
    "Our model is based on an article that analyzes the continuous behavior of mice in a free-roam environment. Precisely, mice are placed into an operant box where if a lever is pressed for a certain threshold of duration (800ms) then the mouse is rewarded with sucrose or food pellets. Behavioral data is collected regarding the number of successful lever presses and the duration of each lever press. Neural data is also recorded in terms of intracellular calcium ion concetration via GCaMP protein photometry. \n",
    "\n",
    "Because both behavioral and neural data are time-series data that share many similarities with sequntial data that is commonly processed with transformer models, we propose to construct a encoder-decoder transformer model to: \n",
    "1. predict the subsequence sequence of neural activity given previous neural recordings. \n",
    "2. Translate abstract neural data prediction in to concrete behavioral predictions via a sigmoid classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "Our input data takes the form of both neural and behavioral time sereis data. Specifically, we use lever press duration as the behavioral component input while we use the corresponding GCaMP protein recording per second for the neural component input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer Model\n",
    "\n",
    "### Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(\n",
    "    nn.Module,\n",
    "):\n",
    "    # needs to go at the bottom of the encoder and decoder stacks\n",
    "    def __init__(self, d_model, dropout: float = 0.1, max_len: int = 3000):\n",
    "        super().__init__()\n",
    "\n",
    "        # Hyperparameter: to introduce regularization that prevents against overfitting\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        pe = torch.squeeze(pe)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(0), :].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder & Decoder\n",
    "The encoder-decoder model will take neural data as encoding input and behavioral data as decoder input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    # input var\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_input_variables,\n",
    "        dim_val,\n",
    "        encoder_dim_feedforward_d_model_scalar,\n",
    "        encoder_dropout,\n",
    "        encoder_num_layers,\n",
    "        encoder_activation,\n",
    "        encoder_num_heads,\n",
    "        decoder_dim_feedforward_d_model_scalar,\n",
    "        decoder_dropout,\n",
    "        decoder_num_layers,\n",
    "        decoder_num_heads,\n",
    "        decoder_activation,\n",
    "        max_seq,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_input_layer = nn.Linear(in_features=max_seq, out_features=dim_val)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(dim_val, max_len=max_seq)\n",
    "\n",
    "        # dim_feedforward must be a scalar of d_model value\n",
    "        # dropout: since a larger dropout is needed within the hidden layers, ensure that it can be up to .5\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_val,\n",
    "            nhead=encoder_num_heads,\n",
    "            dim_feedforward=dim_val * encoder_dim_feedforward_d_model_scalar,\n",
    "            dropout=encoder_dropout,\n",
    "            activation=encoder_activation,\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=encoder_num_layers)\n",
    "\n",
    "        self.decoder_input_layer = nn.Linear(in_features=num_input_variables, out_features=dim_val)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=dim_val,\n",
    "            nhead=decoder_num_heads,\n",
    "            dim_feedforward=dim_val * decoder_dim_feedforward_d_model_scalar,\n",
    "            dropout=decoder_dropout,\n",
    "            activation=decoder_activation,\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer=decoder_layer, num_layers=decoder_num_layers)\n",
    "\n",
    "        self.linear_mapping = nn.Linear(in_features=dim_val, out_features=1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        source = self.encoder_input_layer(source)\n",
    "        source = self.positional_encoding(source)\n",
    "        source = self.encoder(src=source)\n",
    "        target = self.decoder_input_layer(target)\n",
    "        target = self.decoder(tgt=target, memory=source)\n",
    "        target = self.linear_mapping(target)\n",
    "        target = self.sigmoid(target)\n",
    "        return target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Average training Loss: 470.82569460392\n",
      "Training ROCAUC Score: 0.5171567252247322\n",
      "Test ROCAUC Score: 0.679280110752192\n",
      "-------------------------------------\n",
      "Average training Loss: 445.2486722826958\n",
      "Training ROCAUC Score: 0.533437057054792\n",
      "Test ROCAUC Score: 0.6871763318463826\n",
      "-------------------------------------\n",
      "Average training Loss: 435.88584603190424\n",
      "Training ROCAUC Score: 0.5388551881330006\n",
      "Test ROCAUC Score: 0.7089614418294621\n",
      "-------------------------------------\n",
      "Average training Loss: 424.2795119726658\n",
      "Training ROCAUC Score: 0.5753610937625037\n",
      "Test ROCAUC Score: 0.7401537583961442\n",
      "-------------------------------------\n",
      "Average training Loss: 419.84583684682843\n",
      "Training ROCAUC Score: 0.5890366203297881\n",
      "Test ROCAUC Score: 0.7401537583961442\n",
      "-------------------------------------\n",
      "Average training Loss: 413.52537541270254\n",
      "Training ROCAUC Score: 0.623932309464\n",
      "Test ROCAUC Score: 0.7401537583961442\n",
      "-------------------------------------\n",
      "Average training Loss: 409.00398334145547\n",
      "Training ROCAUC Score: 0.6431539124508017\n",
      "Test ROCAUC Score: 0.7401537583961442\n",
      "-------------------------------------\n",
      "Average training Loss: 404.18421001553537\n",
      "Training ROCAUC Score: 0.6558156262561133\n",
      "Test ROCAUC Score: 0.7577183638414603\n",
      "-------------------------------------\n",
      "Average training Loss: 401.7848223340511\n",
      "Training ROCAUC Score: 0.6755193927793205\n",
      "Test ROCAUC Score: 0.8182314900271753\n",
      "-------------------------------------\n",
      "Average training Loss: 396.7121373605728\n",
      "Training ROCAUC Score: 0.7020449179209021\n",
      "Test ROCAUC Score: 0.8182314900271753\n",
      "-------------------------------------\n",
      "Average training Loss: 387.75653214097025\n",
      "Training ROCAUC Score: 0.7357263680539523\n",
      "Test ROCAUC Score: 0.8796579372404245\n",
      "-------------------------------------\n",
      "Average training Loss: 382.04939207434654\n",
      "Training ROCAUC Score: 0.7567064606418608\n",
      "Test ROCAUC Score: 0.8796579372404245\n",
      "-------------------------------------\n",
      "Average training Loss: 381.0638848304749\n",
      "Training ROCAUC Score: 0.7688972325726211\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 379.2177728569508\n",
      "Training ROCAUC Score: 0.7725033412038267\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 372.4134288871288\n",
      "Training ROCAUC Score: 0.7894521897417454\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 371.0302903187275\n",
      "Training ROCAUC Score: 0.8097709401269152\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 364.3471673476696\n",
      "Training ROCAUC Score: 0.8315722376085719\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 359.9338214278221\n",
      "Training ROCAUC Score: 0.8406312460735681\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 358.3925159716606\n",
      "Training ROCAUC Score: 0.8515714465733919\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 353.8695249176025\n",
      "Training ROCAUC Score: 0.8681282727930809\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 349.181520178318\n",
      "Training ROCAUC Score: 0.8753703838268259\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 345.0139667451382\n",
      "Training ROCAUC Score: 0.8892564545251351\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 339.4642007791996\n",
      "Training ROCAUC Score: 0.9020985427476331\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 336.31964520573615\n",
      "Training ROCAUC Score: 0.904446905442782\n",
      "Test ROCAUC Score: 0.9349619289340102\n",
      "-------------------------------------\n",
      "Average training Loss: 331.55296514868735\n",
      "Training ROCAUC Score: 0.919733660294541\n",
      "Test ROCAUC Score: 0.945360970107163\n",
      "-------------------------------------\n",
      "Average training Loss: 327.3102436006069\n",
      "Training ROCAUC Score: 0.9237068644377256\n",
      "Test ROCAUC Score: 0.9614001692047377\n",
      "-------------------------------------\n",
      "Average training Loss: 321.82905120253565\n",
      "Training ROCAUC Score: 0.929164455294095\n",
      "Test ROCAUC Score: 0.9614001692047376\n",
      "-------------------------------------\n",
      "Average training Loss: 322.2290742969513\n",
      "Training ROCAUC Score: 0.9362964652684966\n",
      "Test ROCAUC Score: 0.9614001692047377\n",
      "-------------------------------------\n",
      "Average training Loss: 317.45260759353636\n",
      "Training ROCAUC Score: 0.9420152816959058\n",
      "Test ROCAUC Score: 0.9614001692047377\n",
      "-------------------------------------\n",
      "Average training Loss: 313.72482890605926\n",
      "Training ROCAUC Score: 0.9426431428747505\n",
      "Test ROCAUC Score: 0.9614001692047377\n",
      "-------------------------------------\n",
      "Average training Loss: 309.60299188315867\n",
      "Training ROCAUC Score: 0.9467637922962371\n",
      "Test ROCAUC Score: 0.9614001692047377\n",
      "-------------------------------------\n",
      "Average training Loss: 305.26745809853077\n",
      "Training ROCAUC Score: 0.9472602128620481\n",
      "Test ROCAUC Score: 0.9614001692047377\n",
      "-------------------------------------\n",
      "Average training Loss: 300.39193784594534\n",
      "Training ROCAUC Score: 0.954156935860844\n",
      "Test ROCAUC Score: 0.9614001692047376\n",
      "-------------------------------------\n",
      "Average training Loss: 296.9261685824394\n",
      "Training ROCAUC Score: 0.9626372009128179\n",
      "Test ROCAUC Score: 0.9792019176536944\n",
      "-------------------------------------\n",
      "Average training Loss: 292.88875248849394\n",
      "Training ROCAUC Score: 0.966696315155774\n",
      "Test ROCAUC Score: 0.9792019176536942\n",
      "-------------------------------------\n",
      "Average training Loss: 287.88183238744733\n",
      "Training ROCAUC Score: 0.9658245208028455\n",
      "Test ROCAUC Score: 0.9792019176536942\n",
      "-------------------------------------\n",
      "Average training Loss: 286.0928991341591\n",
      "Training ROCAUC Score: 0.9717385205618557\n",
      "Test ROCAUC Score: 0.9792019176536942\n",
      "-------------------------------------\n",
      "Average training Loss: 282.67576224565505\n",
      "Training ROCAUC Score: 0.9727706375099683\n",
      "Test ROCAUC Score: 0.9792019176536944\n",
      "-------------------------------------\n",
      "Average training Loss: 278.9394845622778\n",
      "Training ROCAUC Score: 0.9781638417819263\n",
      "Test ROCAUC Score: 0.9792019176536942\n",
      "-------------------------------------\n",
      "Average training Loss: 273.4987393897772\n",
      "Training ROCAUC Score: 0.9794912172099821\n",
      "Test ROCAUC Score: 0.9792019176536944\n",
      "-------------------------------------\n",
      "Average training Loss: 269.20953127145765\n",
      "Training ROCAUC Score: 0.9803118702187211\n",
      "Test ROCAUC Score: 0.9792019176536942\n",
      "-------------------------------------\n",
      "Average training Loss: 268.3624935925007\n",
      "Training ROCAUC Score: 0.9791446334241797\n",
      "Test ROCAUC Score: 0.9792019176536944\n",
      "-------------------------------------\n",
      "Average training Loss: 261.3711474096775\n",
      "Training ROCAUC Score: 0.9845600970581769\n",
      "Test ROCAUC Score: 0.9897772137619854\n",
      "-------------------------------------\n",
      "Average training Loss: 275.66517253935336\n",
      "Training ROCAUC Score: 0.964027399251092\n",
      "Test ROCAUC Score: 1.0\n",
      "-------------------------------------\n",
      "Average training Loss: 301.1590565538406\n",
      "Training ROCAUC Score: 0.9885195040761307\n",
      "Test ROCAUC Score: 1.0\n",
      "-------------------------------------\n",
      "Average training Loss: 285.3648675107956\n",
      "Training ROCAUC Score: 0.9914478059351554\n",
      "Test ROCAUC Score: 1.0\n",
      "-------------------------------------\n",
      "Average training Loss: 275.84942220568655\n",
      "Training ROCAUC Score: 0.9933873138192927\n",
      "Test ROCAUC Score: 1.0\n",
      "-------------------------------------\n",
      "Average training Loss: 269.1339696961641\n",
      "Training ROCAUC Score: 0.994363782362307\n",
      "Test ROCAUC Score: 1.0\n",
      "-------------------------------------\n",
      "Average training Loss: 265.3032669657469\n",
      "Training ROCAUC Score: 0.9955002975580007\n",
      "Test ROCAUC Score: 1.0\n",
      "-------------------------------------\n",
      "Average training Loss: 259.8360551917553\n",
      "Training ROCAUC Score: 0.9957606033206922\n",
      "Test ROCAUC Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "dim_val = 512\n",
    "n_heads = 8\n",
    "n_decoder_layers = 4\n",
    "n_encoder_layers = 4\n",
    "input_size = 1\n",
    "max_seq = 703\n",
    "epochs = 50\n",
    "lr = 0.0001\n",
    "loss_function = torch.nn.BCELoss()\n",
    "\n",
    "trainingset = pd.DataFrame(pd.read_csv(\"train_data.csv\"))\n",
    "testset = pd.DataFrame(pd.read_csv(\"test_data.csv\"))\n",
    "trainingset = trainingset[[\"subject\", \"day\", \"lp_met\", \"gcamp_lp_per_sec\"]]\n",
    "testset = testset[[\"subject\", \"day\", \"lp_met\", \"gcamp_lp_per_sec\"]]\n",
    "# drop infs\n",
    "trainingset = trainingset[~trainingset.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "testset = testset[~testset.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "\n",
    "# Get dimensions down\n",
    "trainingset[\"gcamp_lp_per_sec\"] = trainingset[\"gcamp_lp_per_sec\"].round(-2).astype(\"float32\")\n",
    "testset[\"gcamp_lp_per_sec\"] = testset[\"gcamp_lp_per_sec\"].round(-2).astype(\"float32\")\n",
    "\n",
    "## Divide dataset by trials\n",
    "gb = trainingset.groupby([\"subject\", \"day\"])\n",
    "trials = [gb.get_group(x) for x in gb.groups]\n",
    "\n",
    "model = Transformer(\n",
    "    num_input_variables=1,\n",
    "    dim_val=dim_val,\n",
    "    encoder_dim_feedforward_d_model_scalar=5,\n",
    "    encoder_dropout=0.5,\n",
    "    encoder_num_layers=n_encoder_layers,\n",
    "    encoder_activation=\"relu\",\n",
    "    encoder_num_heads=n_heads,\n",
    "    decoder_dim_feedforward_d_model_scalar=5,\n",
    "    decoder_dropout=0.5,\n",
    "    decoder_num_layers=n_decoder_layers,\n",
    "    decoder_num_heads=n_heads,\n",
    "    decoder_activation=\"relu\",\n",
    "    max_seq=max_seq,\n",
    ")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train_loss = 0\n",
    "    target_list = []\n",
    "    predicted_list = []\n",
    "    model.train()\n",
    "    for trial in trials:\n",
    "        optimizer.zero_grad()\n",
    "        source = torch.tensor(trial[\"gcamp_lp_per_sec\"][1:].values)\n",
    "        # Padding source to max lenght of sequence\n",
    "        source = F.pad(source, pad=(0, max_seq + 1 - len(trial)), mode=\"constant\", value=0)\n",
    "        residual = source\n",
    "        target = torch.tensor(trial[\"lp_met\"][1:].values)\n",
    "        target = target.unsqueeze(1).type(torch.FloatTensor)\n",
    "        target_list = target_list + torch.squeeze(target).tolist()\n",
    "        predicted_lp = model(source=source, target=target)\n",
    "        predicted_list = predicted_list + torch.squeeze(predicted_lp).tolist()\n",
    "        try:\n",
    "            loss = loss_function(predicted_lp, target)\n",
    "        except:\n",
    "            #print(predicted_lp[0], model.linear_mapping.weight.grad)\n",
    "            #sys.exit(0)\n",
    "            raise SystemExit(\"Vanishing gradients.\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * source.size(0)\n",
    "\n",
    "    print(\"-------------------------------------\")\n",
    "    print(\"Average training Loss:\", train_loss / len(trials))\n",
    "    print(\"Training ROCAUC Score:\", roc_auc_score(np.array(target_list), np.array(predicted_list)))\n",
    "    ## Calculate ROCAUC testset\n",
    "    model.eval()\n",
    "    gb = testset.groupby([\"subject\", \"day\"])\n",
    "    test_trials = [gb.get_group(x) for x in gb.groups]\n",
    "    target_test_list = []\n",
    "    predicted_test_list = []\n",
    "    for set in test_trials:\n",
    "        source_test = torch.tensor(set[\"gcamp_lp_per_sec\"][1:].values)\n",
    "        source_test = F.pad(source_test, pad=(0, max_seq + 1 - len(set)), mode=\"constant\", value=0)\n",
    "        target_test = torch.tensor(set[\"lp_met\"][1:].values)\n",
    "        target_test = target_test.unsqueeze(1).type(torch.FloatTensor)\n",
    "        target_test_list = target_test_list + torch.squeeze(target_test).tolist()\n",
    "        with torch.no_grad():\n",
    "            predicted_test = model(source=source_test, target=target_test)\n",
    "            predicted_test_list = predicted_test_list + torch.squeeze(predicted_test).tolist()\n",
    "    print(\"Test ROCAUC Score:\", roc_auc_score(np.array(target_test_list), np.array(predicted_test_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "We evaluate the model using an ROC-AUC method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:760: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  y_true = y_true == pos_label\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jorgerag/Documents/UCSD/courses/capstone/Presentation.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jorgerag/Documents/UCSD/courses/capstone/Presentation.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jorgerag/Documents/UCSD/courses/capstone/Presentation.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m#lp_met = np.array(['1', '1', '0', '1', '1'])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jorgerag/Documents/UCSD/courses/capstone/Presentation.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m#Scores = np.array([0.9, 0.8, 0.7, 0.6, 0.55])\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jorgerag/Documents/UCSD/courses/capstone/Presentation.ipynb#X20sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m cal_auc(target_test_list, predicted_test_list)\n",
      "\u001b[1;32m/Users/jorgerag/Documents/UCSD/courses/capstone/Presentation.ipynb Cell 13\u001b[0m in \u001b[0;36mcal_auc\u001b[0;34m(lp_met, Scores)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jorgerag/Documents/UCSD/courses/capstone/Presentation.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcal_auc\u001b[39m(lp_met: np\u001b[39m.\u001b[39mndarray, Scores: np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jorgerag/Documents/UCSD/courses/capstone/Presentation.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     fpr, tpr, thresholds \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39;49mroc_curve(y_true\u001b[39m=\u001b[39;49mlp_met, y_score\u001b[39m=\u001b[39;49mScores, pos_label\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m'\u001b[39;49m, drop_intermediate\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jorgerag/Documents/UCSD/courses/capstone/Presentation.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     roc_point \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mFPR\u001b[39m\u001b[39m'\u001b[39m: fpr, \u001b[39m'\u001b[39m\u001b[39mTPR\u001b[39m\u001b[39m'\u001b[39m: tpr, \u001b[39m'\u001b[39m\u001b[39mthresholds\u001b[39m\u001b[39m'\u001b[39m: thresholds})\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jorgerag/Documents/UCSD/courses/capstone/Presentation.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(roc_point)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:981\u001b[0m, in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mroc_curve\u001b[39m(\n\u001b[1;32m    893\u001b[0m     y_true, y_score, \u001b[39m*\u001b[39m, pos_label\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, drop_intermediate\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    894\u001b[0m ):\n\u001b[1;32m    895\u001b[0m     \u001b[39m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \n\u001b[1;32m    897\u001b[0m \u001b[39m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    979\u001b[0m \n\u001b[1;32m    980\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m     fps, tps, thresholds \u001b[39m=\u001b[39m _binary_clf_curve(\n\u001b[1;32m    982\u001b[0m         y_true, y_score, pos_label\u001b[39m=\u001b[39;49mpos_label, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[1;32m    983\u001b[0m     )\n\u001b[1;32m    985\u001b[0m     \u001b[39m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[1;32m    986\u001b[0m     \u001b[39m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[1;32m    987\u001b[0m     \u001b[39m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[39m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[1;32m    993\u001b[0m     \u001b[39m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[1;32m    994\u001b[0m     \u001b[39mif\u001b[39;00m drop_intermediate \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(fps) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:765\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    763\u001b[0m desc_score_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(y_score, kind\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmergesort\u001b[39m\u001b[39m\"\u001b[39m)[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    764\u001b[0m y_score \u001b[39m=\u001b[39m y_score[desc_score_indices]\n\u001b[0;32m--> 765\u001b[0m y_true \u001b[39m=\u001b[39m y_true[desc_score_indices]\n\u001b[1;32m    766\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    767\u001b[0m     weight \u001b[39m=\u001b[39m sample_weight[desc_score_indices]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def cal_auc(lp_met: np.ndarray, Scores: np.ndarray):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true=lp_met, y_score=Scores, pos_label='1', drop_intermediate=False)\n",
    "\n",
    "    roc_point = pd.DataFrame({'FPR': fpr, 'TPR': tpr, 'thresholds': thresholds})\n",
    "    print(roc_point)\n",
    "\n",
    "    # print('AUC : %s'%metrics.auc(fpr,tpr))\n",
    "    print('AUC: %s' % metrics.roc_auc_score(y_true=lp_met, y_score=Scores))\n",
    "\n",
    "    fig = plt.figure(dpi=120)\n",
    "    plt.plot(fpr, tpr, label='ROC curve')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#lp_met = np.array(['1', '1', '0', '1', '1'])\n",
    "#Scores = np.array([0.9, 0.8, 0.7, 0.6, 0.55])\n",
    "\n",
    "cal_auc(target_test_list, predicted_test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results & Discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
