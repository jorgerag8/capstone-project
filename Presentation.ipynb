{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinky and the Brain: Transforming and Translating Neural Activity with Neural Networks\n",
    "\n",
    "Predicting mice behavior by translating neural data encoding to behavioral decodings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*note: the following are packages used in this project.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from Transformer import Transformer\n",
    "from positional_encoding import PositionalEncoding\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from roc_auc import cal_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Plan: \n",
    "Our model is based on an article that analyzes the continuous behavior of mice in a free-roam environment. Precisely, mice are placed into an operant box where if a lever is pressed for a certain threshold of duration (800ms) then the mouse is rewarded with sucrose or food pellets. Behavioral data is collected regarding the number of successful lever presses and the duration of each lever press. Neural data is also recorded in terms of intracellular calcium ion concetration via GCaMP protein photometry. \n",
    "\n",
    "Because both behavioral and neural data are time-series data that share many similarities with sequntial data that is commonly processed with transformer models, we propose to construct a encoder-decoder transformer model to: \n",
    "1. predict the subsequence sequence of neural activity given previous neural recordings. \n",
    "2. Translate abstract neural data prediction in to concrete behavioral predictions via a sigmoid classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "Our input data takes the form of both neural and behavioral time sereis data. Specifically, we use lever press duration as the behavioral component input while we use the corresponding GCaMP protein recording per second for the neural component input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer Model\n",
    "\n",
    "### Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(\n",
    "    nn.Module,\n",
    "):\n",
    "    # needs to go at the bottom of the encoder and decoder stacks\n",
    "    def __init__(self, d_model, dropout: float = 0.1, max_len: int = 3000):\n",
    "        super().__init__()\n",
    "\n",
    "        # Hyperparameter: to introduce regularization that prevents against overfitting\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        pe = torch.squeeze(pe)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(0), :].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder & Decoder\n",
    "The encoder-decoder model will take neural data as encoding input and behavioral data as decoder input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    # input var\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_input_variables,\n",
    "        dim_val,\n",
    "        encoder_dim_feedforward_d_model_scalar,\n",
    "        encoder_dropout,\n",
    "        encoder_num_layers,\n",
    "        encoder_activation,\n",
    "        encoder_num_heads,\n",
    "        decoder_dim_feedforward_d_model_scalar,\n",
    "        decoder_dropout,\n",
    "        decoder_num_layers,\n",
    "        decoder_num_heads,\n",
    "        decoder_activation,\n",
    "        max_seq,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_input_layer = nn.Linear(in_features=max_seq, out_features=dim_val)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(dim_val, max_len=max_seq)\n",
    "\n",
    "        # dim_feedforward must be a scalar of d_model value\n",
    "        # dropout: since a larger dropout is needed within the hidden layers, ensure that it can be up to .5\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_val,\n",
    "            nhead=encoder_num_heads,\n",
    "            dim_feedforward=dim_val * encoder_dim_feedforward_d_model_scalar,\n",
    "            dropout=encoder_dropout,\n",
    "            activation=encoder_activation,\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=encoder_num_layers)\n",
    "\n",
    "        self.decoder_input_layer = nn.Linear(in_features=num_input_variables, out_features=dim_val)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=dim_val,\n",
    "            nhead=decoder_num_heads,\n",
    "            dim_feedforward=dim_val * decoder_dim_feedforward_d_model_scalar,\n",
    "            dropout=decoder_dropout,\n",
    "            activation=decoder_activation,\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer=decoder_layer, num_layers=decoder_num_layers)\n",
    "\n",
    "        self.linear_mapping = nn.Linear(in_features=dim_val, out_features=1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        source = self.encoder_input_layer(source)\n",
    "        source = self.positional_encoding(source)\n",
    "        source = self.encoder(src=source)\n",
    "        target = self.decoder_input_layer(target)\n",
    "        target = self.decoder(tgt=target, memory=source)\n",
    "        target = self.linear_mapping(target)\n",
    "        target = self.sigmoid(target)\n",
    "        return target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Average training Loss: 470.82569460392\n",
      "Training ROCAUC Score: 0.5171567252247322\n",
      "Test ROCAUC Score: 0.679280110752192\n",
      "-------------------------------------\n",
      "Average training Loss: 445.2486722826958\n",
      "Training ROCAUC Score: 0.533437057054792\n",
      "Test ROCAUC Score: 0.6871763318463826\n",
      "-------------------------------------\n",
      "Average training Loss: 435.88584603190424\n",
      "Training ROCAUC Score: 0.5388551881330006\n",
      "Test ROCAUC Score: 0.7089614418294621\n",
      "-------------------------------------\n",
      "Average training Loss: 424.2795119726658\n",
      "Training ROCAUC Score: 0.5753610937625037\n",
      "Test ROCAUC Score: 0.7401537583961442\n",
      "-------------------------------------\n",
      "Average training Loss: 419.84583684682843\n",
      "Training ROCAUC Score: 0.5890366203297881\n",
      "Test ROCAUC Score: 0.7401537583961442\n",
      "-------------------------------------\n",
      "Average training Loss: 413.52537541270254\n",
      "Training ROCAUC Score: 0.623932309464\n",
      "Test ROCAUC Score: 0.7401537583961442\n",
      "-------------------------------------\n",
      "Average training Loss: 409.00398334145547\n",
      "Training ROCAUC Score: 0.6431539124508017\n",
      "Test ROCAUC Score: 0.7401537583961442\n",
      "-------------------------------------\n",
      "Average training Loss: 404.18421001553537\n",
      "Training ROCAUC Score: 0.6558156262561133\n",
      "Test ROCAUC Score: 0.7577183638414603\n",
      "-------------------------------------\n",
      "Average training Loss: 401.7848223340511\n",
      "Training ROCAUC Score: 0.6755193927793205\n",
      "Test ROCAUC Score: 0.8182314900271753\n",
      "-------------------------------------\n",
      "Average training Loss: 396.7121373605728\n",
      "Training ROCAUC Score: 0.7020449179209021\n",
      "Test ROCAUC Score: 0.8182314900271753\n",
      "-------------------------------------\n",
      "Average training Loss: 387.75653214097025\n",
      "Training ROCAUC Score: 0.7357263680539523\n",
      "Test ROCAUC Score: 0.8796579372404245\n",
      "-------------------------------------\n",
      "Average training Loss: 382.04939207434654\n",
      "Training ROCAUC Score: 0.7567064606418608\n",
      "Test ROCAUC Score: 0.8796579372404245\n",
      "-------------------------------------\n",
      "Average training Loss: 381.0638848304749\n",
      "Training ROCAUC Score: 0.7688972325726211\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 379.2177728569508\n",
      "Training ROCAUC Score: 0.7725033412038267\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 372.4134288871288\n",
      "Training ROCAUC Score: 0.7894521897417454\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 371.0302903187275\n",
      "Training ROCAUC Score: 0.8097709401269152\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 364.3471673476696\n",
      "Training ROCAUC Score: 0.8315722376085719\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 359.9338214278221\n",
      "Training ROCAUC Score: 0.8406312460735681\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 358.3925159716606\n",
      "Training ROCAUC Score: 0.8515714465733919\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 353.8695249176025\n",
      "Training ROCAUC Score: 0.8681282727930809\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 349.181520178318\n",
      "Training ROCAUC Score: 0.8753703838268259\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 345.0139667451382\n",
      "Training ROCAUC Score: 0.8892564545251351\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 339.4642007791996\n",
      "Training ROCAUC Score: 0.9020985427476331\n",
      "Test ROCAUC Score: 0.9075462749320617\n",
      "-------------------------------------\n",
      "Average training Loss: 336.31964520573615\n",
      "Training ROCAUC Score: 0.904446905442782\n",
      "Test ROCAUC Score: 0.9349619289340102\n"
     ]
    }
   ],
   "source": [
    "dim_val = 512\n",
    "n_heads = 8\n",
    "n_decoder_layers = 4\n",
    "n_encoder_layers = 4\n",
    "input_size = 1\n",
    "max_seq = 703\n",
    "epochs = 50\n",
    "lr = 0.0001\n",
    "loss_function = torch.nn.BCELoss()\n",
    "\n",
    "trainingset = pd.DataFrame(pd.read_csv(\"train_data.csv\"))\n",
    "testset = pd.DataFrame(pd.read_csv(\"test_data.csv\"))\n",
    "trainingset = trainingset[[\"subject\", \"day\", \"lp_met\", \"gcamp_lp_per_sec\"]]\n",
    "testset = testset[[\"subject\", \"day\", \"lp_met\", \"gcamp_lp_per_sec\"]]\n",
    "# drop infs\n",
    "trainingset = trainingset[~trainingset.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "testset = testset[~testset.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "\n",
    "# Get dimensions down\n",
    "trainingset[\"gcamp_lp_per_sec\"] = trainingset[\"gcamp_lp_per_sec\"].round(-2).astype(\"float32\")\n",
    "testset[\"gcamp_lp_per_sec\"] = testset[\"gcamp_lp_per_sec\"].round(-2).astype(\"float32\")\n",
    "\n",
    "## Divide dataset by trials\n",
    "gb = trainingset.groupby([\"subject\", \"day\"])\n",
    "trials = [gb.get_group(x) for x in gb.groups]\n",
    "\n",
    "model = Transformer(\n",
    "    num_input_variables=1,\n",
    "    dim_val=dim_val,\n",
    "    encoder_dim_feedforward_d_model_scalar=5,\n",
    "    encoder_dropout=0.5,\n",
    "    encoder_num_layers=n_encoder_layers,\n",
    "    encoder_activation=\"relu\",\n",
    "    encoder_num_heads=n_heads,\n",
    "    decoder_dim_feedforward_d_model_scalar=5,\n",
    "    decoder_dropout=0.5,\n",
    "    decoder_num_layers=n_decoder_layers,\n",
    "    decoder_num_heads=n_heads,\n",
    "    decoder_activation=\"relu\",\n",
    "    max_seq=max_seq,\n",
    ")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train_loss = 0\n",
    "    target_list = []\n",
    "    predicted_list = []\n",
    "    model.train()\n",
    "    for trial in trials:\n",
    "        optimizer.zero_grad()\n",
    "        source = torch.tensor(trial[\"gcamp_lp_per_sec\"][1:].values)\n",
    "        # Padding source to max lenght of sequence\n",
    "        source = F.pad(source, pad=(0, max_seq + 1 - len(trial)), mode=\"constant\", value=0)\n",
    "        residual = source\n",
    "        target = torch.tensor(trial[\"lp_met\"][1:].values)\n",
    "        target = target.unsqueeze(1).type(torch.FloatTensor)\n",
    "        target_list = target_list + torch.squeeze(target).tolist()\n",
    "        predicted_lp = model(source=source, target=target)\n",
    "        predicted_list = predicted_list + torch.squeeze(predicted_lp).tolist()\n",
    "        try:\n",
    "            loss = loss_function(predicted_lp, target)\n",
    "        except:\n",
    "            #print(predicted_lp[0], model.linear_mapping.weight.grad)\n",
    "            #sys.exit(0)\n",
    "            raise SystemExit(\"Vanishing gradients.\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * source.size(0)\n",
    "\n",
    "    print(\"-------------------------------------\")\n",
    "    print(\"Average training Loss:\", train_loss / len(trials))\n",
    "    print(\"Training ROCAUC Score:\", roc_auc_score(np.array(target_list), np.array(predicted_list)))\n",
    "    ## Calculate ROCAUC testset\n",
    "    model.eval()\n",
    "    gb = testset.groupby([\"subject\", \"day\"])\n",
    "    test_trials = [gb.get_group(x) for x in gb.groups]\n",
    "    target_test_list = []\n",
    "    predicted_test_list = []\n",
    "    for set in test_trials:\n",
    "        source_test = torch.tensor(set[\"gcamp_lp_per_sec\"][1:].values)\n",
    "        source_test = F.pad(source_test, pad=(0, max_seq + 1 - len(set)), mode=\"constant\", value=0)\n",
    "        target_test = torch.tensor(set[\"lp_met\"][1:].values)\n",
    "        target_test = target_test.unsqueeze(1).type(torch.FloatTensor)\n",
    "        target_test_list = target_test_list + torch.squeeze(target_test).tolist()\n",
    "        with torch.no_grad():\n",
    "            predicted_test = model(source=source_test, target=target_test)\n",
    "            predicted_test_list = predicted_test_list + torch.squeeze(predicted_test).tolist()\n",
    "    print(\"Test ROCAUC Score:\", roc_auc_score(np.array(target_test_list), np.array(predicted_test_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "We evaluate the model using an ROC-AUC method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_auc(lp_met: np.ndarray, Scores: np.ndarray):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true=lp_met, y_score=Scores, pos_label='1', drop_intermediate=False)\n",
    "\n",
    "    roc_point = pd.DataFrame({'FPR': fpr, 'TPR': tpr, 'thresholds': thresholds})\n",
    "    print(roc_point)\n",
    "\n",
    "    # print('AUC : %s'%metrics.auc(fpr,tpr))\n",
    "    print('AUC: %s' % metrics.roc_auc_score(y_true=lp_met, y_score=Scores))\n",
    "\n",
    "    fig = plt.figure(dpi=120)\n",
    "    plt.plot(fpr, tpr, label='ROC curve')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#lp_met = np.array(['1', '1', '0', '1', '1'])\n",
    "#Scores = np.array([0.9, 0.8, 0.7, 0.6, 0.55])\n",
    "\n",
    "cal_auc(np.array(target_test_list), np.array(predicted_test_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results & Discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
